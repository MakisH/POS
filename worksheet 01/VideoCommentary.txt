4.1.2

The function CalcHourglassControlForElems() takes 72% of the execution time, and the function CalcKinematicsForElems() takes 22% of the time.  Combined, they take up 94% of the execution time. Almost all of the time is spent in the functions themselves not in child functions of the functions.

4.2.2

1. For icc, we have 17+24+8+49+19 = 117 optimization flags in 5 categories (Optimization, Code generation,
 Interprocedural Optimization (IPO), Advanced Optimizations, and Profile Guided Optimization (PGO)). For gcc, there are approximately 230 optimization flags.

2. No, it is not practical to test all the flags manually. Instead, we could follow
basic guidelines from the vendors, or use tools that implement search algorithms,
like Periscope. We actually tried running all the gcc flag combinations with a brute force method implimented in a self made python script,
and the running time was several hours.

3. In our case, these flags were -O3 -march="native" -funroll-loops and -floop-block"
for gcc, and -03 -xHost -unroll for icc.

4.3.2

The ivdep pragma tells the compiler that it can avoid any assumed dependences, but it will still apply performance heuristics to decide if to vectorize or not. The vector  prgrama (for example vector always) will ask the compiler to always vectorize the loop despite the heuristics, if it is legal. The simd pragma will tell the compiler to avoid both assumed dependencies and heuristics.

In the function CalcHourglassControlForElems, there is an outer OpenMP loop and an inner loop with length 8. The loop can be vectorized, but the overhead to use the SIMD registers is too large for such a small loop, as our tests showed. We tried to use vectorization with and without OpenMP and we found out that in the general (with OpenMP) case, the vectorization was decreasing the performance. So, we decided to use ivdep for the Intel compiler, to let the compiler decide in every case. Moreover, we didn't observe any better results with other loop transformations or inlining. In the case of gcc, none of the pragmas we tried gave better results.

5.1.2

Near-linear scalability was achieved in up to 8 threads. Thre was relatively little performance increase from 8 to 16 threads
because at 16 threads, the threads are spread over two sockets.
The maximum performance was achieved at 16 threads.

5.2.2

LULESH only accepts cubes of integers (i.e. That means, 1, 8, 27, etc.) as process counts.
We want to keep one process per core. To use over 16 processes, one must use multiple nodes, in order to keep one process per core.
We didn't have linear scalability, we had data that asymptotically approaches a maximum FOM at 64 processes and then starts to decrease slightly. Above 8 processes we have multiple processes per core, but we still have some improvement as process number increases because of hyperthreading.

The maximum process count was achieved at 64.
The performance and scalability of the MPI results look similar to the OpenMP results.

5.3.2

The valid combinations are one process with one, two, four, eight, or sixteen threads, or eight processes
with one or two threads. This is because Lulesh only allows integer cubes for process numbers, and on one node, to avoid having more than 1 process per core, the product of processes and threads must be less than or equal to 16.

We have non-linear scalability in the truly hybrid MPI / OpenMP case, because we showed in previous answers that MPI and OpenMP do not have linear scalability, and combining both of them does not change this non-linear scalability.

If we run the hybrid process with one thread and one process, it is slower than the OpenMP or the MPI case because there is more overhead. However, for greater numbers of processes, the hybrid helps because it combines the advantages of both MPI and OpenMP.

The hybrid solution is overall the fastest.

We would not have guesseed this combination because we have two combinations that use all the cores on one node. One is with one process and sixteen threads. The other is with eight processes and two threads. We wouldn't know the relative effect of OpenMP or MPI overheads before doing the experiments.
