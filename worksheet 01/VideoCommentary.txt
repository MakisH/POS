4.1.2: Nathan

"In the video, make sure to mention which functions represented most of the execution time of the benchmark. Enumerate the functions that make together 80% or more of the execution time."

The function CalcHourglassControlForElems() takes 72% of the execution time, and the function CalcKinematicsForElems() takes 22% of the time.  Combined, they take up 94% of the execution time. 

4.2.2: Chip
1. where to find

2. it is not practical to test all the flags.  In this case, a step by step 
perfomance optimization is most practical.  First run w/o any compiler flags.
Then, impliment general optimization step by step with the flag -01 -02 -03.
Intel then recomends initializing processor specific optimizations, namely
"-xHost". Then apply code specific geusses one by one to test for performace 
optimizations, and impliment only those where a higher performace is achieved.

2. In our cases these flags were -O3 -march="native" -funroll-loops -floop-block"
for gcc and -03 -xHost -unroll for icc
4.3.2: Makis
5.1.2: Chip

In this case linear scalability was not achieved but rather relative speedup 
decread as the number of threads increased

5.2.2: Nathan

"
To configure the benchmark with MPI enabled, please refer to section 3.2.2. Build the benchmark and launch it with the provided Load-Leveler script.
Take a look at the benchmark’s documentation and evaluate scalability with valid process counts. The number of processes must not exceed the total of cores in the node used. Record the performance with each process count and generate a plot. Use GNU Plot or generate and export a plot from a spreadsheet."

"Submit the scalability plot in PDF format. Name the file: ’MPI_scalability.pdf’."

"
Comment about the scalability observed:
• What are the valid combinations of processes allowed?
• Was linear scalability achieved?
• On which process-count was the maximum performance achieved?
• How does the performance compare to the results achieved with OpenMP in section 5.1? "

LULESH only accepts cubes of integers (i.e. That means, 1, 8, 27, etc.) as process counts, and SuperMUC thin nodes have 16 cores total.

27 tasks, 27 processes, node = 1: srv04-ib.1068896:

Elapsed time         =     290.62 (s)
Grind time (us/z/c)  =  3.4224587 (per dom)  (0.12675773 overall)
FOM                  =  7889.0653 (z/s)

8 tasks, 8 processes: srv03-ib.1093468:

Elapsed time         =     101.06 (s)
Grind time (us/z/c)  =  1.8428662 (per dom)  (0.23035827 overall)
FOM                  =   4341.064 (z/s)

1 task, 1 process: srv04-ib.1068897:

Elapsed time         =      43.35 (s)
Grind time (us/z/c)  =   1.722695 (per dom)  (  1.722695 overall)
FOM                  =  580.48581 (z/s)

27 tasks, 27 processes, node = 3: srv04-ib.1068898:

Elapsed time         =     158.70 (s)
Grind time (us/z/c)  =  1.8689763 (per dom)  (0.069221343 overall)
FOM                  =  14446.411 (z/s)

16 tasks, 8 processes: srv04-ib.1068902:

Elapsed time         =     100.91 (s)
Grind time (us/z/c)  =  1.8402649 (per dom)  (0.23003311 overall)
FOM                  =  4347.2003 (z/s)

16 tasks, 1 process: srv03-ib.1093472:

Elapsed time         =      43.11 (s)
Grind time (us/z/c)  =  1.7130081 (per dom)  ( 1.7130081 overall)
FOM                  =  583.76841 (z/s)


5.3.2: Makis
